<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Detecting faces in Video contents using Azure Cognitive Services Face API | Yoichi Kawasaki</title><meta name=keywords content="Python,AzureMediaServices,AzureMediaAnalytics,CognitiveServices"><meta name=description content="過去に本ブログでビデオコンテンツを切り口とした音声認識やOCR技術を利用したデモを紹介したが、ここではビデオコンテンツの中の人物出現箇所に連動して人物名を字幕で表示させるデモとその実装方法を紹介したい。人物識別にはAzureのCognitive ServicesのFace APIを使っていて、これで動画の中に出現する顔の検出を行い、予め登録している人物リストとのマッチングにより実現している。 Cognitive Serivcesとは視覚、音声、言語、知識などマイクロソフトがこれまで研究を通じて開発してきたさまざまな要素技術をAPIとして提供しているサービスのことで、最近巷で人工知能（AI）だとかインテリジェンスとかいうキーワードをよく耳にするのではないかと思うがAzure利用シナリオでそういったインテリジェンス（知能/知性）を兼ね備えたアプリを作る場合は間違いなく中核となるサービスの1つである。Face APIはその中でも顔の検出・識別や、顔にまつわる感情、特徴などメタデータ抽出に特化したAPIである。

 demo site source code  主要テクノロジーと機能 下図は今回のデモ作成のために行っている処理フローと主要テクノロジーを表している。やっていることは大きく分けて3つ: (1) 動画コンテンツをAzure Media Encoder Standardを使ってフレームごとの静止画像の作成, (2) Cognitive ServicesのFace APIを使って1より得られた静止画像から顔の検出を行い予め登録している人物リストとマッチング（最も類似度が高いものを本人とみなす）して人物を識別, (3) 2で得られた各フレーム中の人物情報を時間順に並べて字幕(Closed Caption)用のデータファイルを生成。以下、各処理の詳細について説明する。
1. Azure Media Encoder Standardでフレームごとの静止画生成 残念ながらFace APIはビデオコンテンツから直接顔検出することができないため、一旦ビデオコンテンツから各フレームごとの静止画を生成してその静止画を対象に処理を行う必要がある。ここでは各フレームごとの静止画生成にAzure Media Encoder Standard（MES）を利用する。MESを使うことでエンコードタスクとしてビデオコンテンツに対して様々な処理を行うことができるのだが、MESにはそのエンコードタスクの１つとしてサムネイル生成のためのタスクが用意されており、今回はこのサムネール生成タスクを利用する。他のエンコードタスク同様にサムネイル生成タスクについてもプリセットと呼ばれるエンコードに必要な情報を記述した XML または JSON形式ファイルを用意する必要がある。今回は1秒フレームごとにJPEG形式の静止画（サムネイル）を生成するために次のようなプリセット（amsmp-thumbnail-config.json）を用意した。
{  &#34;Version&#34;: 1.0,  &#34;Codecs&#34;: [  {  &#34;Start&#34;: &#34;00:00:00&#34;,  &#34;Step&#34;: &#34;00:00:01&#34;,  &#34;Type&#34;: &#34;JpgImage&#34;,  &#34;JpgLayers&#34;: [  {  &#34;Quality&#34;: 90,  &#34;Type&#34;: &#34;JpgLayer&#34;,  &#34;Width&#34;: 640,  &#34;Height&#34;: 360  }  ]  }  ],  &#34;Outputs&#34;: [  {  &#34;FileName&#34;: &#34;{Basename}_{Index}{Extension}&#34;,  &#34;Format&#34;: {  &#34;Type&#34;: &#34;JpgFormat&#34;  }  }  ] } MESによるサムネイル処理実行方法やプリセットの詳細については「Media Encoder Standard を使用した高度なエンコード」や同ページの「サムネイルを生成する」項を参照ください。尚、今回のサムネイル生成のためのエンコーディング処理は小生自作の「azure-media-processor-java」を利用してバッチ実行している。"><meta name=author content="Yoichi Kawasaki"><link rel=canonical href=https://unofficialism.info/posts/azure-media-cognitive-demos-video-frames-face-recognition/><link crossorigin=anonymous href=https://unofficialism.info/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=https://unofficialism.info/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=icon type=image/png sizes=16x16 href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=icon type=image/png sizes=32x32 href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=apple-touch-icon href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=mask-icon href=https://unofficialism.info/Thinking_Face_Emoji_large.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Detecting faces in Video contents using Azure Cognitive Services Face API"><meta property="og:description" content="過去に本ブログでビデオコンテンツを切り口とした音声認識やOCR技術を利用したデモを紹介したが、ここではビデオコンテンツの中の人物出現箇所に連動して人物名を字幕で表示させるデモとその実装方法を紹介したい。人物識別にはAzureのCognitive ServicesのFace APIを使っていて、これで動画の中に出現する顔の検出を行い、予め登録している人物リストとのマッチングにより実現している。 Cognitive Serivcesとは視覚、音声、言語、知識などマイクロソフトがこれまで研究を通じて開発してきたさまざまな要素技術をAPIとして提供しているサービスのことで、最近巷で人工知能（AI）だとかインテリジェンスとかいうキーワードをよく耳にするのではないかと思うがAzure利用シナリオでそういったインテリジェンス（知能/知性）を兼ね備えたアプリを作る場合は間違いなく中核となるサービスの1つである。Face APIはその中でも顔の検出・識別や、顔にまつわる感情、特徴などメタデータ抽出に特化したAPIである。

 demo site source code  主要テクノロジーと機能 下図は今回のデモ作成のために行っている処理フローと主要テクノロジーを表している。やっていることは大きく分けて3つ: (1) 動画コンテンツをAzure Media Encoder Standardを使ってフレームごとの静止画像の作成, (2) Cognitive ServicesのFace APIを使って1より得られた静止画像から顔の検出を行い予め登録している人物リストとマッチング（最も類似度が高いものを本人とみなす）して人物を識別, (3) 2で得られた各フレーム中の人物情報を時間順に並べて字幕(Closed Caption)用のデータファイルを生成。以下、各処理の詳細について説明する。
1. Azure Media Encoder Standardでフレームごとの静止画生成 残念ながらFace APIはビデオコンテンツから直接顔検出することができないため、一旦ビデオコンテンツから各フレームごとの静止画を生成してその静止画を対象に処理を行う必要がある。ここでは各フレームごとの静止画生成にAzure Media Encoder Standard（MES）を利用する。MESを使うことでエンコードタスクとしてビデオコンテンツに対して様々な処理を行うことができるのだが、MESにはそのエンコードタスクの１つとしてサムネイル生成のためのタスクが用意されており、今回はこのサムネール生成タスクを利用する。他のエンコードタスク同様にサムネイル生成タスクについてもプリセットと呼ばれるエンコードに必要な情報を記述した XML または JSON形式ファイルを用意する必要がある。今回は1秒フレームごとにJPEG形式の静止画（サムネイル）を生成するために次のようなプリセット（amsmp-thumbnail-config.json）を用意した。
{  &#34;Version&#34;: 1.0,  &#34;Codecs&#34;: [  {  &#34;Start&#34;: &#34;00:00:00&#34;,  &#34;Step&#34;: &#34;00:00:01&#34;,  &#34;Type&#34;: &#34;JpgImage&#34;,  &#34;JpgLayers&#34;: [  {  &#34;Quality&#34;: 90,  &#34;Type&#34;: &#34;JpgLayer&#34;,  &#34;Width&#34;: 640,  &#34;Height&#34;: 360  }  ]  }  ],  &#34;Outputs&#34;: [  {  &#34;FileName&#34;: &#34;{Basename}_{Index}{Extension}&#34;,  &#34;Format&#34;: {  &#34;Type&#34;: &#34;JpgFormat&#34;  }  }  ] } MESによるサムネイル処理実行方法やプリセットの詳細については「Media Encoder Standard を使用した高度なエンコード」や同ページの「サムネイルを生成する」項を参照ください。尚、今回のサムネイル生成のためのエンコーディング処理は小生自作の「azure-media-processor-java」を利用してバッチ実行している。"><meta property="og:type" content="article"><meta property="og:url" content="https://unofficialism.info/posts/azure-media-cognitive-demos-video-frames-face-recognition/"><meta property="og:image" content="https://unofficialism.info/profile.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-12-18T22:21:16+00:00"><meta property="article:modified_time" content="2016-12-18T22:21:16+00:00"><meta property="og:site_name" content="Yoichi Kawasaki"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://unofficialism.info/profile.jpg"><meta name=twitter:title content="Detecting faces in Video contents using Azure Cognitive Services Face API"><meta name=twitter:description content="過去に本ブログでビデオコンテンツを切り口とした音声認識やOCR技術を利用したデモを紹介したが、ここではビデオコンテンツの中の人物出現箇所に連動して人物名を字幕で表示させるデモとその実装方法を紹介したい。人物識別にはAzureのCognitive ServicesのFace APIを使っていて、これで動画の中に出現する顔の検出を行い、予め登録している人物リストとのマッチングにより実現している。 Cognitive Serivcesとは視覚、音声、言語、知識などマイクロソフトがこれまで研究を通じて開発してきたさまざまな要素技術をAPIとして提供しているサービスのことで、最近巷で人工知能（AI）だとかインテリジェンスとかいうキーワードをよく耳にするのではないかと思うがAzure利用シナリオでそういったインテリジェンス（知能/知性）を兼ね備えたアプリを作る場合は間違いなく中核となるサービスの1つである。Face APIはその中でも顔の検出・識別や、顔にまつわる感情、特徴などメタデータ抽出に特化したAPIである。

 demo site source code  主要テクノロジーと機能 下図は今回のデモ作成のために行っている処理フローと主要テクノロジーを表している。やっていることは大きく分けて3つ: (1) 動画コンテンツをAzure Media Encoder Standardを使ってフレームごとの静止画像の作成, (2) Cognitive ServicesのFace APIを使って1より得られた静止画像から顔の検出を行い予め登録している人物リストとマッチング（最も類似度が高いものを本人とみなす）して人物を識別, (3) 2で得られた各フレーム中の人物情報を時間順に並べて字幕(Closed Caption)用のデータファイルを生成。以下、各処理の詳細について説明する。
1. Azure Media Encoder Standardでフレームごとの静止画生成 残念ながらFace APIはビデオコンテンツから直接顔検出することができないため、一旦ビデオコンテンツから各フレームごとの静止画を生成してその静止画を対象に処理を行う必要がある。ここでは各フレームごとの静止画生成にAzure Media Encoder Standard（MES）を利用する。MESを使うことでエンコードタスクとしてビデオコンテンツに対して様々な処理を行うことができるのだが、MESにはそのエンコードタスクの１つとしてサムネイル生成のためのタスクが用意されており、今回はこのサムネール生成タスクを利用する。他のエンコードタスク同様にサムネイル生成タスクについてもプリセットと呼ばれるエンコードに必要な情報を記述した XML または JSON形式ファイルを用意する必要がある。今回は1秒フレームごとにJPEG形式の静止画（サムネイル）を生成するために次のようなプリセット（amsmp-thumbnail-config.json）を用意した。
{  &#34;Version&#34;: 1.0,  &#34;Codecs&#34;: [  {  &#34;Start&#34;: &#34;00:00:00&#34;,  &#34;Step&#34;: &#34;00:00:01&#34;,  &#34;Type&#34;: &#34;JpgImage&#34;,  &#34;JpgLayers&#34;: [  {  &#34;Quality&#34;: 90,  &#34;Type&#34;: &#34;JpgLayer&#34;,  &#34;Width&#34;: 640,  &#34;Height&#34;: 360  }  ]  }  ],  &#34;Outputs&#34;: [  {  &#34;FileName&#34;: &#34;{Basename}_{Index}{Extension}&#34;,  &#34;Format&#34;: {  &#34;Type&#34;: &#34;JpgFormat&#34;  }  }  ] } MESによるサムネイル処理実行方法やプリセットの詳細については「Media Encoder Standard を使用した高度なエンコード」や同ページの「サムネイルを生成する」項を参照ください。尚、今回のサムネイル生成のためのエンコーディング処理は小生自作の「azure-media-processor-java」を利用してバッチ実行している。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://unofficialism.info/posts/"},{"@type":"ListItem","position":3,"name":"Detecting faces in Video contents using Azure Cognitive Services Face API","item":"https://unofficialism.info/posts/azure-media-cognitive-demos-video-frames-face-recognition/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Detecting faces in Video contents using Azure Cognitive Services Face API","name":"Detecting faces in Video contents using Azure Cognitive Services Face API","description":"過去に本ブログでビデオコンテンツを切り口とした音声認識やOCR技術を利用したデモを紹介したが、ここではビデオコンテンツの中の人物出現箇所に連動して人物名を字幕で表示させるデモとその実装方法を紹介したい。人物識別にはAzureのCognitive ServicesのFace APIを使っていて、これで動画の中に出現する顔の検出を行い、予め登録している人物リストとのマッチングにより実現している。 Cognitive Serivcesとは視覚、音声、言語、知識などマイクロソフトがこれまで研究を通じて開発してきたさまざまな要素技術をAPIとして提供しているサービスのことで、最近巷で人工知能（AI）だとかインテリジェンスとかいうキーワードをよく耳にするのではないかと思うがAzure利用シナリオでそういったインテリジェンス（知能/知性）を兼ね備えたアプリを作る場合は間違いなく中核となるサービスの1つである。Face APIはその中でも顔の検出・識別や、顔にまつわる感情、特徴などメタデータ抽出に特化したAPIである。\n\n demo site source code  主要テクノロジーと機能 下図は今回のデモ作成のために行っている処理フローと主要テクノロジーを表している。やっていることは大きく分けて3つ: (1) 動画コンテンツをAzure Media Encoder Standardを使ってフレームごとの静止画像の作成, (2) Cognitive ServicesのFace APIを使って1より得られた静止画像から顔の検出を行い予め登録している人物リストとマッチング（最も類似度が高いものを本人とみなす）して人物を識別, (3) 2で得られた各フレーム中の人物情報を時間順に並べて字幕(Closed Caption)用のデータファイルを生成。以下、各処理の詳細について説明する。\n1. Azure Media Encoder Standardでフレームごとの静止画生成 残念ながらFace APIはビデオコンテンツから直接顔検出することができないため、一旦ビデオコンテンツから各フレームごとの静止画を生成してその静止画を対象に処理を行う必要がある。ここでは各フレームごとの静止画生成にAzure Media Encoder Standard（MES）を利用する。MESを使うことでエンコードタスクとしてビデオコンテンツに対して様々な処理を行うことができるのだが、MESにはそのエンコードタスクの１つとしてサムネイル生成のためのタスクが用意されており、今回はこのサムネール生成タスクを利用する。他のエンコードタスク同様にサムネイル生成タスクについてもプリセットと呼ばれるエンコードに必要な情報を記述した XML または JSON形式ファイルを用意する必要がある。今回は1秒フレームごとにJPEG形式の静止画（サムネイル）を生成するために次のようなプリセット（amsmp-thumbnail-config.json）を用意した。\n{  \u0026#34;Version\u0026#34;: 1.0,  \u0026#34;Codecs\u0026#34;: [  {  \u0026#34;Start\u0026#34;: \u0026#34;00:00:00\u0026#34;,  \u0026#34;Step\u0026#34;: \u0026#34;00:00:01\u0026#34;,  \u0026#34;Type\u0026#34;: \u0026#34;JpgImage\u0026#34;,  \u0026#34;JpgLayers\u0026#34;: [  {  \u0026#34;Quality\u0026#34;: 90,  \u0026#34;Type\u0026#34;: \u0026#34;JpgLayer\u0026#34;,  \u0026#34;Width\u0026#34;: 640,  \u0026#34;Height\u0026#34;: 360  }  ]  }  ],  \u0026#34;Outputs\u0026#34;: [  {  \u0026#34;FileName\u0026#34;: \u0026#34;{Basename}_{Index}{Extension}\u0026#34;,  \u0026#34;Format\u0026#34;: {  \u0026#34;Type\u0026#34;: \u0026#34;JpgFormat\u0026#34;  }  }  ] } MESによるサムネイル処理実行方法やプリセットの詳細については「Media Encoder Standard を使用した高度なエンコード」や同ページの「サムネイルを生成する」項を参照ください。尚、今回のサムネイル生成のためのエンコーディング処理は小生自作の「azure-media-processor-java」を利用してバッチ実行している。","keywords":["Python","AzureMediaServices","AzureMediaAnalytics","CognitiveServices"],"articleBody":"過去に本ブログでビデオコンテンツを切り口とした音声認識やOCR技術を利用したデモを紹介したが、ここではビデオコンテンツの中の人物出現箇所に連動して人物名を字幕で表示させるデモとその実装方法を紹介したい。人物識別にはAzureのCognitive ServicesのFace APIを使っていて、これで動画の中に出現する顔の検出を行い、予め登録している人物リストとのマッチングにより実現している。 Cognitive Serivcesとは視覚、音声、言語、知識などマイクロソフトがこれまで研究を通じて開発してきたさまざまな要素技術をAPIとして提供しているサービスのことで、最近巷で人工知能（AI）だとかインテリジェンスとかいうキーワードをよく耳にするのではないかと思うがAzure利用シナリオでそういったインテリジェンス（知能/知性）を兼ね備えたアプリを作る場合は間違いなく中核となるサービスの1つである。Face APIはその中でも顔の検出・識別や、顔にまつわる感情、特徴などメタデータ抽出に特化したAPIである。\n\n demo site source code  主要テクノロジーと機能 下図は今回のデモ作成のために行っている処理フローと主要テクノロジーを表している。やっていることは大きく分けて3つ: (1) 動画コンテンツをAzure Media Encoder Standardを使ってフレームごとの静止画像の作成, (2) Cognitive ServicesのFace APIを使って1より得られた静止画像から顔の検出を行い予め登録している人物リストとマッチング（最も類似度が高いものを本人とみなす）して人物を識別, (3) 2で得られた各フレーム中の人物情報を時間順に並べて字幕(Closed Caption)用のデータファイルを生成。以下、各処理の詳細について説明する。\n1. Azure Media Encoder Standardでフレームごとの静止画生成 残念ながらFace APIはビデオコンテンツから直接顔検出することができないため、一旦ビデオコンテンツから各フレームごとの静止画を生成してその静止画を対象に処理を行う必要がある。ここでは各フレームごとの静止画生成にAzure Media Encoder Standard（MES）を利用する。MESを使うことでエンコードタスクとしてビデオコンテンツに対して様々な処理を行うことができるのだが、MESにはそのエンコードタスクの１つとしてサムネイル生成のためのタスクが用意されており、今回はこのサムネール生成タスクを利用する。他のエンコードタスク同様にサムネイル生成タスクについてもプリセットと呼ばれるエンコードに必要な情報を記述した XML または JSON形式ファイルを用意する必要がある。今回は1秒フレームごとにJPEG形式の静止画（サムネイル）を生成するために次のようなプリセット（amsmp-thumbnail-config.json）を用意した。\n{  \"Version\": 1.0,  \"Codecs\": [  {  \"Start\": \"00:00:00\",  \"Step\": \"00:00:01\",  \"Type\": \"JpgImage\",  \"JpgLayers\": [  {  \"Quality\": 90,  \"Type\": \"JpgLayer\",  \"Width\": 640,  \"Height\": 360  }  ]  }  ],  \"Outputs\": [  {  \"FileName\": \"{Basename}_{Index}{Extension}\",  \"Format\": {  \"Type\": \"JpgFormat\"  }  }  ] } MESによるサムネイル処理実行方法やプリセットの詳細については「Media Encoder Standard を使用した高度なエンコード」や同ページの「サムネイルを生成する」項を参照ください。尚、今回のサムネイル生成のためのエンコーディング処理は小生自作の「azure-media-processor-java」を利用してバッチ実行している。\n2. Cognitive Services Face APIによる顔の検出と人物の識別 ここではCognitive ServicesのFace APIを使って１で得られたフレームごとの静止画像に対して顔検出を行い、予め登録している人物リスト（Face APIでいうところのPerson Group）と比較して最も類似度の高い人物（Face APIでいうところのPerson ）をその本人として識別する。\n2-1. 人物リスト（Person Group）の作成 人物リスト（Person Group）の作成で必要な作業とFace APIの利用インターフェースは次の通り:\n Create a Person Group APIを使って Person Groupを作成 上記で作成したPerson Groupの中にCreate a Person APIで人物ごとにPersonを作成する。作成されたPerson対してAdd a Person Face APIでその人物の顔画像を登録する。Face APIでは各Personに最大248枚の顔画像を登録が可能となっており、さまざまな種類の顔を登録することで機械学習によりその人物の顔識別の精度が向上するとされている。 上記でPerson Groupに対して登録されたPersonデータ（Personごとの顔データ）は最終的にTrain Person Group APIでトレーニングされることで、次の2-2で行う顔識別（Face Identify API）処理で利用可能なデータとなる。注意点として、いくらある人物の顔画像を登録したとしてもそれがトレーニングされない限り顔識別処理において有効にはならないため、新しく顔を登録した場合はトレーニング処理を忘れずに行ってください（この手のことは自動化しておいてください）。  2-2. 静止画像中の顔認識と人物識別 ここで行う処理の流れとFace APIの利用インターフェースは次の通り:\n Face Detect APIを使って静止画像中の顔を検出する。検出された顔ごとに固有のIDが得られる。尚、1枚の画像で複数の顔が検出された場合、最大64までは取得可能となっている（2016年12月現在）。 上記の顔検出で得られた顔IDを元にFace Identify APIを使って2-1で登録した人物リスト（Person Group）に対して人物検索を行い顔の類似度（0～1の数値）が高いもの順に一覧を取得することができる。ここでは最も類似度が高い人物をその顔の人物として決定する。  3. 字幕(Closed Caption)データファイルの生成 2で得られた各フレーム中の人物情報と各フレームの時間を元に字幕用のデータフォーマットであるWebVTTフォーマットファイルを生成する。以下、6秒～30秒までの字幕出力を期したWebVTTファイルのサンプルであるが、見ていただいてわかる通りフレームの時間（最小秒単位）とそこで得られた人物名をセットで記述するとても単純なフォーマットとなっている。\n00:00:06.000 -- 00:00:07.000 Satya Nadella(0.73295)` 00:00:07.000 -- 00:00:08.000 Satya Nadella(0.6313) 00:00:27.000 -- 00:00:28.000 Bryan Roper(0.68094) 00:00:29.000 -- 00:00:30.000 Bryan Roper(0.54939) 各フレームの時間について、今回のビデオコンテンツのフレームは1秒ごとに取得しており、フレームごとの静止画像ファイルにはフレームの順番がPostfixとしてファイル名に含まれているため単純にファイル名からフレームの時間が特定できるようになっている（例, 10番目のファイル= videoassetname_000010.jpg）。もし今回のような機械的なルールがない場合はフレーム用画像ファイル名と時間のマッピングが必要となる。\nビデオコンテンツと字幕の再生は「ビデオコンテンツの音声認識デモ」でも紹介したようにHTML5のtrackタグエレメントによるビデオファイルの字幕表示機能使って人物名の字幕表示を実現している。本デモではHTML5に下記のようにビデオファイル（MP4）をVideoソースとしてtrackエレメントにWebVTTファイル（build2016keynote.vtt）を指定している。\nvideo id=\"Video1\" controls autoplay width=\"600\"  source src=\"KEY01_VideoThumbnail.mp4\" srclang=\"en\" type=\"video/mp4\"  track id=\"trackJA\" src=\"build2016keynote.vtt\" kind=\"captions\" srclang=\"ja\" label=\"Person Name\" default video デモデータ作成手順 GithubプロジェクトページVideoFramesFaceRecognition-Pythonの1. Preparationと2. Batch executionを実施いただければFace APIで識別した各フレームごとの人物名を元に字幕データ*.vttファイルが生成されデモページ表示のための準備は完了する。最後に表示用の静的ページを生成すれば完了。本デモの表示用ページデータはこちらで、基本的にindex.htmlの変更のみでいけるはず。\n本デモコンテンツについて何か問題を発見した場合はこちらのGithub IssueページにIssueとして登録いただけると幸いである。\nAzure Media Analytics Face Detectorを活用した処理の効率化 今回の人物識別ではビデオコンテンツの全てのフレームに対してFace APIを使って顔検出処理を行っているが、これでは顔出現フレームが少ないコンテンツの場合には無駄なFace APIリクエストが大量に発生してしまうため効率的な処理とは言えない。ということで、ここではAzure Media Face Detectorを活用して処理を効率化する方法を紹介したい。\nAzure Media Face DetectorはAzure Media Servicesのメディアプロセッサ(MP)の１つで、ビデオコンテンツから顔の検出や感情の検出をすることができる。残念ながらAzure Media Face DetectorはFace APIのように顔の識別を行うことはできないものの、ビデオコンテンツから直接顔を検出することができる、即ちビデオコンテンツから直接顔が存在するフレームを特定することができる。よって、この機能を利用して一旦Azure Media Face Detectorで顔が検出されたフレームのみに絞り込んでからFace APIを使ってフレームの静止画像に対して顔検出・顔識別を行うことで無駄なFace APIリクエストを減らして処理の効率化を図ることができる。処理フローとしては次のようなイメージ。\nおまけ: Video Summarization デモページをみていただくとお分かりのように今回のデモでは3分のビデオコンテンツを題材としているが、元ネタはChannel9で公開されている計138分のBuild 2016のキーノートセッションである。このキーノートのセッションはデモコンテンツとしてはあまりに長かったのでこれをAzure Media Video Thumbnailsメディアプロセッサ（MP）を使って3分に要約している。Azure Media Video Thumbnailsはアルゴリズムベースで特徴シーンの検出とそれらを結合（サブクリップ）してビデオコンテンツを指定した長さに要約することができるMPで、現在Public Previewリリース中(2016年12月現在)。\n参考までに、要約（3分:180秒）に使用したAzure Media Video Thumbnailsのタスクプリセットは以下の通り：\n{  \"version\": \"1.0\",  \"options\": {  \"outputAudio\": \"true\",  \"maxMotionThumbnailDurationInSecs\": \"180\",  \"fadeInFadeOut\": \"true\"  } } END\n","wordCount":"244","inLanguage":"en","datePublished":"2016-12-18T22:21:16Z","dateModified":"2016-12-18T22:21:16Z","author":{"@type":"Person","name":"Yoichi Kawasaki"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://unofficialism.info/posts/azure-media-cognitive-demos-video-frames-face-recognition/"},"publisher":{"@type":"Organization","name":"Yoichi Kawasaki","logo":{"@type":"ImageObject","url":"https://unofficialism.info/Thinking_Face_Emoji_large.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://unofficialism.info accesskey=h title="unofficialism (Alt + H)">unofficialism</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://unofficialism.info/posts title=posts><span>posts</span></a></li><li><a href=https://unofficialism.info/works title=works><span>works</span></a></li><li><a href=https://unofficialism.info/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://unofficialism.info/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://unofficialism.info>Home</a>&nbsp;»&nbsp;<a href=https://unofficialism.info/posts/>Posts</a></div><h1 class=post-title>Detecting faces in Video contents using Azure Cognitive Services Face API</h1><div class=post-meta><span title="2016-12-18 00:00:00 +0000 UTC">December 18, 2016</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Yoichi Kawasaki&nbsp;|&nbsp;<a href=https://github.com/yokawasa/yokawasa.github.io/tree/master/content/posts/2016-12-18-azure-media-cognitive-demos-video-frames-face-recognition.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>過去に本ブログでビデオコンテンツを切り口とした<a href=http://unofficialism.info/posts/azure-media-cognitive-demos-video-ocr-speech-to-text/>音声認識</a>や<a href=http://unofficialism.info/posts/azure-media-cognitive-demos-video-ocr/>OCR</a>技術を利用したデモを紹介したが、ここではビデオコンテンツの中の人物出現箇所に連動して人物名を字幕で表示させるデモとその実装方法を紹介したい。人物識別にはAzureのCognitive ServicesのFace APIを使っていて、これで動画の中に出現する顔の検出を行い、予め登録している人物リストとのマッチングにより実現している。
<a href=https://azure.microsoft.com/ja-jp/services/cognitive-services/>Cognitive Serivces</a>とは視覚、音声、言語、知識などマイクロソフトがこれまで研究を通じて開発してきたさまざまな要素技術をAPIとして提供しているサービスのことで、最近巷で人工知能（AI）だとかインテリジェンスとかいうキーワードをよく耳にするのではないかと思うがAzure利用シナリオでそういったインテリジェンス（知能/知性）を兼ね備えたアプリを作る場合は間違いなく中核となるサービスの1つである。Face APIはその中でも顔の検出・識別や、顔にまつわる感情、特徴などメタデータ抽出に特化したAPIである。</p><p><a href=http://azure-media-cognitive-demos.azurewebsites.net/faceapi/build2016keynote/><img loading=lazy src=https://c1.staticflickr.com/6/5484/30351964320_dc0b3400ca_b.jpg alt="Video Summarization and Face Detection Demo Screenshot"></a></p><ul><li><a href=http://azure-media-cognitive-demos.azurewebsites.net/faceapi/build2016keynote/>demo site</a></li><li><a href=https://github.com/AzureMediaCognitiveDemos/VideoFramesFaceRecognition-Python>source code</a></li></ul><h2 id=主要テクノロジーと機能>主要テクノロジーと機能<a hidden class=anchor aria-hidden=true href=#主要テクノロジーと機能>#</a></h2><p>下図は今回のデモ作成のために行っている処理フローと主要テクノロジーを表している。やっていることは大きく分けて3つ: (1) 動画コンテンツをAzure Media Encoder Standardを使ってフレームごとの静止画像の作成, (2) Cognitive ServicesのFace APIを使って1より得られた静止画像から顔の検出を行い予め登録している人物リストとマッチング（最も類似度が高いものを本人とみなす）して人物を識別, (3) 2で得られた各フレーム中の人物情報を時間順に並べて字幕(Closed Caption)用のデータファイルを生成。以下、各処理の詳細について説明する。</p><p><img loading=lazy src=https://c6.staticflickr.com/1/590/31711341965_fe98683a31_b.jpg alt=VideoFramesFaceFecognition_case1></p><h3 id=1-azure-media-encoder-standardでフレームごとの静止画生成>1. Azure Media Encoder Standardでフレームごとの静止画生成<a hidden class=anchor aria-hidden=true href=#1-azure-media-encoder-standardでフレームごとの静止画生成>#</a></h3><p>残念ながらFace APIはビデオコンテンツから直接顔検出することができないため、一旦ビデオコンテンツから各フレームごとの静止画を生成してその静止画を対象に処理を行う必要がある。ここでは各フレームごとの静止画生成にAzure Media Encoder Standard（MES）を利用する。MESを使うことでエンコードタスクとしてビデオコンテンツに対して様々な処理を行うことができるのだが、MESにはそのエンコードタスクの１つとしてサムネイル生成のためのタスクが用意されており、今回はこのサムネール生成タスクを利用する。他のエンコードタスク同様にサムネイル生成タスクについてもプリセットと呼ばれるエンコードに必要な情報を記述した XML または JSON形式ファイルを用意する必要がある。今回は1秒フレームごとにJPEG形式の静止画（サムネイル）を生成するために次のようなプリセット（<a href=https://github.com/AzureMediaCognitiveDemos/VideoFramesFaceRecognition-Python/blob/master/src/amsmp-thumbnail-config.json>amsmp-thumbnail-config.json</a>）を用意した。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;Version&#34;</span>: <span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;Codecs&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Start&#34;</span>: <span style=color:#e6db74>&#34;00:00:00&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Step&#34;</span>: <span style=color:#e6db74>&#34;00:00:01&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Type&#34;</span>: <span style=color:#e6db74>&#34;JpgImage&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;JpgLayers&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;Quality&#34;</span>: <span style=color:#ae81ff>90</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;Type&#34;</span>: <span style=color:#e6db74>&#34;JpgLayer&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;Width&#34;</span>: <span style=color:#ae81ff>640</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;Height&#34;</span>: <span style=color:#ae81ff>360</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;Outputs&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;FileName&#34;</span>: <span style=color:#e6db74>&#34;{Basename}_{Index}{Extension}&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Format&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;Type&#34;</span>: <span style=color:#e6db74>&#34;JpgFormat&#34;</span>
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>MESによるサムネイル処理実行方法やプリセットの詳細については「<a href=https://docs.microsoft.com/ja-jp/azure/media-services/media-services-custom-mes-presets-with-dotnet>Media Encoder Standard を使用した高度なエンコード</a>」や同ページの「<a href=https://docs.microsoft.com/ja-jp/azure/media-services/media-services-custom-mes-presets-with-dotnet#thumbnails>サムネイルを生成する</a>」項を参照ください。尚、今回のサムネイル生成のためのエンコーディング処理は小生自作の「<a href=https://github.com/yokawasa/azure-media-processor-java>azure-media-processor-java</a>」を利用してバッチ実行している。</p><h3 id=2-cognitive-services-face-apiによる顔の検出と人物の識別>2. Cognitive Services Face APIによる顔の検出と人物の識別<a hidden class=anchor aria-hidden=true href=#2-cognitive-services-face-apiによる顔の検出と人物の識別>#</a></h3><p>ここではCognitive ServicesのFace APIを使って１で得られたフレームごとの静止画像に対して顔検出を行い、予め登録している人物リスト（Face APIでいうところのPerson Group）と比較して最も類似度の高い人物（Face APIでいうところのPerson ）をその本人として識別する。</p><h4 id=2-1-人物リストperson-groupの作成>2-1. 人物リスト（Person Group）の作成<a hidden class=anchor aria-hidden=true href=#2-1-人物リストperson-groupの作成>#</a></h4><p>人物リスト（Person Group）の作成で必要な作業とFace APIの利用インターフェースは次の通り:</p><ul><li><a href=https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395244>Create a Person Group API</a>を使って Person Groupを作成</li><li>上記で作成したPerson Groupの中に<a href=https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f3039523c>Create a Person API</a>で人物ごとにPersonを作成する。作成されたPerson対して<a href=https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f3039523b>Add a Person Face API</a>でその人物の顔画像を登録する。Face APIでは各Personに最大248枚の顔画像を登録が可能となっており、さまざまな種類の顔を登録することで機械学習によりその人物の顔識別の精度が向上するとされている。</li><li>上記でPerson Groupに対して登録されたPersonデータ（Personごとの顔データ）は最終的に<a href=https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395249>Train Person Group API</a>でトレーニングされることで、次の2-2で行う顔識別（<a href=https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395239>Face Identify API</a>）処理で利用可能なデータとなる。注意点として、いくらある人物の顔画像を登録したとしてもそれがトレーニングされない限り顔識別処理において有効にはならないため、新しく顔を登録した場合はトレーニング処理を忘れずに行ってください（この手のことは自動化しておいてください）。</li></ul><h4 id=2-2-静止画像中の顔認識と人物識別>2-2. 静止画像中の顔認識と人物識別<a hidden class=anchor aria-hidden=true href=#2-2-静止画像中の顔認識と人物識別>#</a></h4><p>ここで行う処理の流れとFace APIの利用インターフェースは次の通り:</p><ul><li><a href=https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236>Face Detect API</a>を使って静止画像中の顔を検出する。検出された顔ごとに固有のIDが得られる。尚、1枚の画像で複数の顔が検出された場合、最大64までは取得可能となっている（2016年12月現在）。</li><li>上記の顔検出で得られた顔IDを元に<a href=https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395239>Face Identify API</a>を使って2-1で登録した人物リスト（Person Group）に対して人物検索を行い顔の類似度（0～1の数値）が高いもの順に一覧を取得することができる。ここでは最も類似度が高い人物をその顔の人物として決定する。</li></ul><h3 id=3-字幕closed-captionデータファイルの生成>3. 字幕(Closed Caption)データファイルの生成<a hidden class=anchor aria-hidden=true href=#3-字幕closed-captionデータファイルの生成>#</a></h3><p>2で得られた各フレーム中の人物情報と各フレームの時間を元に字幕用のデータフォーマットである<a href=https://w3c.github.io/webvtt/>WebVTTフォーマット</a>ファイルを生成する。以下、6秒～30秒までの字幕出力を期したWebVTTファイルのサンプルであるが、見ていただいてわかる通りフレームの時間（最小秒単位）とそこで得られた人物名をセットで記述するとても単純なフォーマットとなっている。</p><pre tabindex=0><code>00:00:06.000 --&gt; 00:00:07.000
Satya Nadella(0.73295)`

00:00:07.000 --&gt; 00:00:08.000
Satya Nadella(0.6313)

00:00:27.000 --&gt; 00:00:28.000
Bryan Roper(0.68094)

00:00:29.000 --&gt; 00:00:30.000
Bryan Roper(0.54939)
</code></pre><p>各フレームの時間について、今回のビデオコンテンツのフレームは1秒ごとに取得しており、フレームごとの静止画像ファイルにはフレームの順番がPostfixとしてファイル名に含まれているため単純にファイル名からフレームの時間が特定できるようになっている（例, 10番目のファイル= videoassetname_000010.jpg）。もし今回のような機械的なルールがない場合はフレーム用画像ファイル名と時間のマッピングが必要となる。</p><p>ビデオコンテンツと字幕の再生は「<a href=http://unofficialism.info/posts/azure-media-cognitive-demos-video-ocr-speech-to-text/>ビデオコンテンツの音声認識デモ</a>」でも紹介したようにHTML5のtrackタグエレメントによるビデオファイルの字幕表示機能使って人物名の字幕表示を実現している。本デモではHTML5に下記のようにビデオファイル（MP4）をVideoソースとしてtrackエレメントにWebVTTファイル（<a href=https://github.com/AzureMediaCognitiveDemos/VideoFramesFaceRecognition-Python/blob/master/demo/build2016keynote/build2016keynote.vtt>build2016keynote.vtt</a>）を指定している。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span>&lt;<span style=color:#f92672>video</span> <span style=color:#a6e22e>id</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Video1&#34;</span> <span style=color:#a6e22e>controls</span> <span style=color:#a6e22e>autoplay</span> <span style=color:#a6e22e>width</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;600&#34;</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#f92672>source</span> <span style=color:#a6e22e>src</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;KEY01_VideoThumbnail.mp4&#34;</span> <span style=color:#a6e22e>srclang</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;en&#34;</span> <span style=color:#a6e22e>type</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;video/mp4&#34;</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#f92672>track</span> <span style=color:#a6e22e>id</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;trackJA&#34;</span>  <span style=color:#a6e22e>src</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;build2016keynote.vtt&#34;</span>  <span style=color:#a6e22e>kind</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;captions&#34;</span> <span style=color:#a6e22e>srclang</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ja&#34;</span> <span style=color:#a6e22e>label</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Person Name&#34;</span> <span style=color:#a6e22e>default</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>video</span>&gt;
</span></span></code></pre></div><h2 id=デモデータ作成手順>デモデータ作成手順<a hidden class=anchor aria-hidden=true href=#デモデータ作成手順>#</a></h2><p>Githubプロジェクトページ<a href=https://github.com/AzureMediaCognitiveDemos/VideoFramesFaceRecognition-Python>VideoFramesFaceRecognition-Python</a>の1. Preparationと2. Batch executionを実施いただければFace APIで識別した各フレームごとの人物名を元に字幕データ*.vttファイルが生成されデモページ表示のための準備は完了する。最後に表示用の静的ページを生成すれば完了。本デモの表示用ページデータは<a href=https://github.com/AzureMediaCognitiveDemos/VideoFramesFaceRecognition-Python/tree/master/demo/build2016keynote>こちら</a>で、基本的にindex.htmlの変更のみでいけるはず。</p><p>本デモコンテンツについて何か問題を発見した場合はこちらの<a href=https://github.com/AzureMediaCognitiveDemos/VideoFramesFaceRecognition-Python/issues>Github Issueページ</a>にIssueとして登録いただけると幸いである。</p><h2 id=azure-media-analytics-face-detectorを活用した処理の効率化>Azure Media Analytics Face Detectorを活用した処理の効率化<a hidden class=anchor aria-hidden=true href=#azure-media-analytics-face-detectorを活用した処理の効率化>#</a></h2><p>今回の人物識別ではビデオコンテンツの全てのフレームに対してFace APIを使って顔検出処理を行っているが、これでは顔出現フレームが少ないコンテンツの場合には無駄なFace APIリクエストが大量に発生してしまうため効率的な処理とは言えない。ということで、ここでは<a href=https://docs.microsoft.com/ja-jp/azure/media-services/media-services-face-and-emotion-detection>Azure Media Face Detector</a>を活用して処理を効率化する方法を紹介したい。</p><p>Azure Media Face DetectorはAzure Media Servicesのメディアプロセッサ(MP)の１つで、ビデオコンテンツから顔の検出や感情の検出をすることができる。残念ながらAzure Media Face DetectorはFace APIのように顔の識別を行うことはできないものの、ビデオコンテンツから直接顔を検出することができる、即ちビデオコンテンツから直接顔が存在するフレームを特定することができる。よって、この機能を利用して一旦Azure Media Face Detectorで顔が検出されたフレームのみに絞り込んでからFace APIを使ってフレームの静止画像に対して顔検出・顔識別を行うことで無駄なFace APIリクエストを減らして処理の効率化を図ることができる。処理フローとしては次のようなイメージ。</p><p><img loading=lazy src=https://c5.staticflickr.com/1/604/31338431900_ff47cbb4d9_b.jpg alt=VideoFramesFaceFecognition_case2></p><h2 id=おまけ-video-summarization>おまけ: Video Summarization<a hidden class=anchor aria-hidden=true href=#おまけ-video-summarization>#</a></h2><p><a href=http://azure-media-cognitive-demos.azurewebsites.net/faceapi/build2016keynote/>デモページ</a>をみていただくとお分かりのように今回のデモでは3分のビデオコンテンツを題材としているが、元ネタは<a href=https://channel9.msdn.com/Events/Build/2016/KEY01>Channel9</a>で公開されている計138分の<a href=https://channel9.msdn.com/Events/Build/2016/KEY01>Build 2016のキーノートセッション</a>である。このキーノートのセッションはデモコンテンツとしてはあまりに長かったのでこれを<a href=https://docs.microsoft.com/ja-jp/azure/media-services/media-services-video-summarization>Azure Media Video Thumbnails</a>メディアプロセッサ（MP）を使って3分に要約している。Azure Media Video Thumbnailsはアルゴリズムベースで特徴シーンの検出とそれらを結合（サブクリップ）してビデオコンテンツを指定した長さに要約することができるMPで、現在Public Previewリリース中(2016年12月現在)。</p><p>参考までに、要約（3分:180秒）に使用したAzure Media Video Thumbnailsのタスクプリセットは以下の通り：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;version&#34;</span>: <span style=color:#e6db74>&#34;1.0&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;options&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;outputAudio&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;maxMotionThumbnailDurationInSecs&#34;</span>: <span style=color:#e6db74>&#34;180&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&#34;fadeInFadeOut&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>END</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://unofficialism.info/tags/python/>Python</a></li><li><a href=https://unofficialism.info/tags/azuremediaservices/>AzureMediaServices</a></li><li><a href=https://unofficialism.info/tags/azuremediaanalytics/>AzureMediaAnalytics</a></li><li><a href=https://unofficialism.info/tags/cognitiveservices/>CognitiveServices</a></li></ul><nav class=paginav><a class=prev href=https://unofficialism.info/posts/logstash-plugins-for-azure-services/><span class=title>« Prev Page</span><br><span>Logstash plugins for Microsoft Azure Services</span></a>
<a class=next href=https://unofficialism.info/posts/fluent-plugin-azurefunctions/><span class=title>Next Page »</span><br><span>Collecting events into Azure Functions and triggering your custom code using fluent-plugin-azurefunctions</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Detecting faces in Video contents using Azure Cognitive Services Face API on twitter" href="https://twitter.com/intent/tweet/?text=Detecting%20faces%20in%20Video%20contents%20using%20Azure%20Cognitive%20Services%20Face%20API&url=https%3a%2f%2funofficialism.info%2fposts%2fazure-media-cognitive-demos-video-frames-face-recognition%2f&hashtags=Python%2cAzureMediaServices%2cAzureMediaAnalytics%2cCognitiveServices"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Detecting faces in Video contents using Azure Cognitive Services Face API on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2funofficialism.info%2fposts%2fazure-media-cognitive-demos-video-frames-face-recognition%2f&title=Detecting%20faces%20in%20Video%20contents%20using%20Azure%20Cognitive%20Services%20Face%20API&summary=Detecting%20faces%20in%20Video%20contents%20using%20Azure%20Cognitive%20Services%20Face%20API&source=https%3a%2f%2funofficialism.info%2fposts%2fazure-media-cognitive-demos-video-frames-face-recognition%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Detecting faces in Video contents using Azure Cognitive Services Face API on reddit" href="https://reddit.com/submit?url=https%3a%2f%2funofficialism.info%2fposts%2fazure-media-cognitive-demos-video-frames-face-recognition%2f&title=Detecting%20faces%20in%20Video%20contents%20using%20Azure%20Cognitive%20Services%20Face%20API"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Detecting faces in Video contents using Azure Cognitive Services Face API on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2funofficialism.info%2fposts%2fazure-media-cognitive-demos-video-frames-face-recognition%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://unofficialism.info>Yoichi Kawasaki</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>