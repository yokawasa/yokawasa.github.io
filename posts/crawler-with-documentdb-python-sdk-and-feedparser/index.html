<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DocumentDB Python SDKとfeedparserで作る簡易クローラー | Yoichi Kawasaki</title><meta name=keywords content="DocumentDB,NoSQL,Python,Crawler,pydocumentdb,feedparser"><meta name=description content="DocumentDB Python SDKとfeedparserを使って簡易クローラーを作りましょうというお話。ここではDocumentDBをクローリング結果の格納先データストアとして使用する。クロール対象はAzure日本語ブログのRSSフィード、これをfeedparserを使ってドキュメント解析、必要データの抽出、そしてその結果を今回使用するpydocumentdbというDocumentDB Python SDKを使ってDocumentDBに格納するというワークフローになっている。
DocumentDB Python SDK - pydocumentdb Azureで提供されているどのサービスにもあてはまることであるが、DocumentDBを操作するための全てのインターフェースはREST APIとして提供されておりREST APIを内部的に使用してマイクロソフト謹製もしくは個人のコントリビューションによる複数の言語のSDKが用意されている。その中でもpydocumentdbはPython用のDocumentDB SDKであり、オープンソースとしてソースコードは全てGithubで公開されている。
 pydocumentdbプロジェクトトップ(Github) pydocumentdbサンプルコード(Github) Azure DocumentDB REST API Reference  Pre-Requirementsその１: Python実行環境とライブラリ 実行環境としてPython2.7系が必要となる。また、今回クローラーが使用しているDocumentDB Python SDKであるpydocumentdbとRSSフィード解析ライブラリfeedparserの２つのライブラリのインストールが必要となる。
pydocumentdbインストール
$ sudo pip install pydocumentdb feedparserインストール
$ sudo pip install feedparser ちなみにpipがインストールされていない場合は下記の通りマニュアルもしくはインストーラーを使用してpipをインストールが必要となる
pip マニュアルインストール
# download get-pip.py $ wget https://bootstrap.pypa.io/get-pip.py # run the following (which may require administrator access) $ sudo python get-pip.py # upgrade pip $ sudo pip install -U pip インストーラーを使用"><meta name=author content="Yoichi Kawasaki"><link rel=canonical href=https://unofficialism.info/posts/crawler-with-documentdb-python-sdk-and-feedparser/><link crossorigin=anonymous href=https://unofficialism.info/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=https://unofficialism.info/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=icon type=image/png sizes=16x16 href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=icon type=image/png sizes=32x32 href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=apple-touch-icon href=https://unofficialism.info/Thinking_Face_Emoji_large.png><link rel=mask-icon href=https://unofficialism.info/Thinking_Face_Emoji_large.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="DocumentDB Python SDKとfeedparserで作る簡易クローラー"><meta property="og:description" content="DocumentDB Python SDKとfeedparserを使って簡易クローラーを作りましょうというお話。ここではDocumentDBをクローリング結果の格納先データストアとして使用する。クロール対象はAzure日本語ブログのRSSフィード、これをfeedparserを使ってドキュメント解析、必要データの抽出、そしてその結果を今回使用するpydocumentdbというDocumentDB Python SDKを使ってDocumentDBに格納するというワークフローになっている。
DocumentDB Python SDK - pydocumentdb Azureで提供されているどのサービスにもあてはまることであるが、DocumentDBを操作するための全てのインターフェースはREST APIとして提供されておりREST APIを内部的に使用してマイクロソフト謹製もしくは個人のコントリビューションによる複数の言語のSDKが用意されている。その中でもpydocumentdbはPython用のDocumentDB SDKであり、オープンソースとしてソースコードは全てGithubで公開されている。
 pydocumentdbプロジェクトトップ(Github) pydocumentdbサンプルコード(Github) Azure DocumentDB REST API Reference  Pre-Requirementsその１: Python実行環境とライブラリ 実行環境としてPython2.7系が必要となる。また、今回クローラーが使用しているDocumentDB Python SDKであるpydocumentdbとRSSフィード解析ライブラリfeedparserの２つのライブラリのインストールが必要となる。
pydocumentdbインストール
$ sudo pip install pydocumentdb feedparserインストール
$ sudo pip install feedparser ちなみにpipがインストールされていない場合は下記の通りマニュアルもしくはインストーラーを使用してpipをインストールが必要となる
pip マニュアルインストール
# download get-pip.py $ wget https://bootstrap.pypa.io/get-pip.py # run the following (which may require administrator access) $ sudo python get-pip.py # upgrade pip $ sudo pip install -U pip インストーラーを使用"><meta property="og:type" content="article"><meta property="og:url" content="https://unofficialism.info/posts/crawler-with-documentdb-python-sdk-and-feedparser/"><meta property="og:image" content="https://unofficialism.info/profile.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2015-06-21T17:32:40+00:00"><meta property="article:modified_time" content="2015-06-21T17:32:40+00:00"><meta property="og:site_name" content="Yoichi Kawasaki"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://unofficialism.info/profile.jpg"><meta name=twitter:title content="DocumentDB Python SDKとfeedparserで作る簡易クローラー"><meta name=twitter:description content="DocumentDB Python SDKとfeedparserを使って簡易クローラーを作りましょうというお話。ここではDocumentDBをクローリング結果の格納先データストアとして使用する。クロール対象はAzure日本語ブログのRSSフィード、これをfeedparserを使ってドキュメント解析、必要データの抽出、そしてその結果を今回使用するpydocumentdbというDocumentDB Python SDKを使ってDocumentDBに格納するというワークフローになっている。
DocumentDB Python SDK - pydocumentdb Azureで提供されているどのサービスにもあてはまることであるが、DocumentDBを操作するための全てのインターフェースはREST APIとして提供されておりREST APIを内部的に使用してマイクロソフト謹製もしくは個人のコントリビューションによる複数の言語のSDKが用意されている。その中でもpydocumentdbはPython用のDocumentDB SDKであり、オープンソースとしてソースコードは全てGithubで公開されている。
 pydocumentdbプロジェクトトップ(Github) pydocumentdbサンプルコード(Github) Azure DocumentDB REST API Reference  Pre-Requirementsその１: Python実行環境とライブラリ 実行環境としてPython2.7系が必要となる。また、今回クローラーが使用しているDocumentDB Python SDKであるpydocumentdbとRSSフィード解析ライブラリfeedparserの２つのライブラリのインストールが必要となる。
pydocumentdbインストール
$ sudo pip install pydocumentdb feedparserインストール
$ sudo pip install feedparser ちなみにpipがインストールされていない場合は下記の通りマニュアルもしくはインストーラーを使用してpipをインストールが必要となる
pip マニュアルインストール
# download get-pip.py $ wget https://bootstrap.pypa.io/get-pip.py # run the following (which may require administrator access) $ sudo python get-pip.py # upgrade pip $ sudo pip install -U pip インストーラーを使用"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://unofficialism.info/posts/"},{"@type":"ListItem","position":3,"name":"DocumentDB Python SDKとfeedparserで作る簡易クローラー","item":"https://unofficialism.info/posts/crawler-with-documentdb-python-sdk-and-feedparser/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DocumentDB Python SDKとfeedparserで作る簡易クローラー","name":"DocumentDB Python SDKとfeedparserで作る簡易クローラー","description":"DocumentDB Python SDKとfeedparserを使って簡易クローラーを作りましょうというお話。ここではDocumentDBをクローリング結果の格納先データストアとして使用する。クロール対象はAzure日本語ブログのRSSフィード、これをfeedparserを使ってドキュメント解析、必要データの抽出、そしてその結果を今回使用するpydocumentdbというDocumentDB Python SDKを使ってDocumentDBに格納するというワークフローになっている。\nDocumentDB Python SDK - pydocumentdb Azureで提供されているどのサービスにもあてはまることであるが、DocumentDBを操作するための全てのインターフェースはREST APIとして提供されておりREST APIを内部的に使用してマイクロソフト謹製もしくは個人のコントリビューションによる複数の言語のSDKが用意されている。その中でもpydocumentdbはPython用のDocumentDB SDKであり、オープンソースとしてソースコードは全てGithubで公開されている。\n pydocumentdbプロジェクトトップ(Github) pydocumentdbサンプルコード(Github) Azure DocumentDB REST API Reference  Pre-Requirementsその１: Python実行環境とライブラリ 実行環境としてPython2.7系が必要となる。また、今回クローラーが使用しているDocumentDB Python SDKであるpydocumentdbとRSSフィード解析ライブラリfeedparserの２つのライブラリのインストールが必要となる。\npydocumentdbインストール\n$ sudo pip install pydocumentdb feedparserインストール\n$ sudo pip install feedparser ちなみにpipがインストールされていない場合は下記の通りマニュアルもしくはインストーラーを使用してpipをインストールが必要となる\npip マニュアルインストール\n# download get-pip.py $ wget https://bootstrap.pypa.io/get-pip.py # run the following (which may require administrator access) $ sudo python get-pip.py # upgrade pip $ sudo pip install -U pip インストーラーを使用","keywords":["DocumentDB","NoSQL","Python","Crawler","pydocumentdb","feedparser"],"articleBody":"DocumentDB Python SDKとfeedparserを使って簡易クローラーを作りましょうというお話。ここではDocumentDBをクローリング結果の格納先データストアとして使用する。クロール対象はAzure日本語ブログのRSSフィード、これをfeedparserを使ってドキュメント解析、必要データの抽出、そしてその結果を今回使用するpydocumentdbというDocumentDB Python SDKを使ってDocumentDBに格納するというワークフローになっている。\nDocumentDB Python SDK - pydocumentdb Azureで提供されているどのサービスにもあてはまることであるが、DocumentDBを操作するための全てのインターフェースはREST APIとして提供されておりREST APIを内部的に使用してマイクロソフト謹製もしくは個人のコントリビューションによる複数の言語のSDKが用意されている。その中でもpydocumentdbはPython用のDocumentDB SDKであり、オープンソースとしてソースコードは全てGithubで公開されている。\n pydocumentdbプロジェクトトップ(Github) pydocumentdbサンプルコード(Github) Azure DocumentDB REST API Reference  Pre-Requirementsその１: Python実行環境とライブラリ 実行環境としてPython2.7系が必要となる。また、今回クローラーが使用しているDocumentDB Python SDKであるpydocumentdbとRSSフィード解析ライブラリfeedparserの２つのライブラリのインストールが必要となる。\npydocumentdbインストール\n$ sudo pip install pydocumentdb feedparserインストール\n$ sudo pip install feedparser ちなみにpipがインストールされていない場合は下記の通りマニュアルもしくはインストーラーを使用してpipをインストールが必要となる\npip マニュアルインストール\n# download get-pip.py $ wget https://bootstrap.pypa.io/get-pip.py # run the following (which may require administrator access) $ sudo python get-pip.py # upgrade pip $ sudo pip install -U pip インストーラーを使用\n# apt-getの場合 $ sudo apt-get install python-pip # yumの場合 $ sudo yum install python-pip Pre-requirementsその２: DocumentDBアカウント Azure新ポータルより下記イメージのフローでDocumentDBアカウントを作成する。\nDocumentDBアカウントを作成したら今度はデータを格納するためのDocumentDBデータベースとコレクションを作成する必要がある。DocumentDBデータベースとコレクションはAzure新ポータルから作成可能であるが、今回のクローラープログラムではpydocumentdbを使って作成するようにしている。\nRSSクローラーとその実行結果 RSSクローラーのソースコード（rssCrawler4Docdb.py）とその実行結果は下記の通り。もしクローリングを定期的に実行する場合はこのスクリプトをジョブスケジューラー（cronなど）に登録ください。指定のRSSフィードに新しく追加された記事のみがDocumentDBに反映されるようスクリプト上は重複チェックを入れている。\n#!/usr/bin/env python # -*- coding: utf-8 -*- import feedparser import pydocumentdb.documents as documents import pydocumentdb.document_client as document_client import pydocumentdb.errors as errors import pydocumentdb.http_constants as http_constants  Docdb_masterKey = '' Docdb_host = 'https://.documents.azure.com:443/' Docdb_dbname = '' Docdb_colname = ''  feedurl='http://blogs.msdn.com/b/windowsazurej/atom.aspx'  def rsscrawling():  # create documentDb client instance  client = document_client.DocumentClient(Docdb_host,  {'masterKey': Docdb_masterKey})  # create a database if not yet created  database_definition = {'id': Docdb_dbname }  databases = list(client.QueryDatabases({  'query': 'SELECT * FROM root r WHERE r.id=@id',  'parameters': [  { 'name':'@id', 'value': database_definition['id'] }  ]  }))   if ( len(databases)  0 ):  feeddb = databases[0]  else:  print \"database is created:%s\" % Docdb_dbname  feeddb = client.CreateDatabase(database_definition)   # create a collection if not yet created  collection_definition = { 'id': Docdb_colname }  collections = list(client.QueryCollections(  feeddb['_self'],  {  'query': 'SELECT * FROM root r WHERE r.id=@id',  'parameters': [  { 'name':'@id', 'value': collection_definition['id'] }  ]  }))  if ( len(collections)  0 ):  collection = collections[0]  else:  print \"collection is created:%s\" % Docdb_colname  collection = client.CreateCollection(  feeddb['_self'], collection_definition)  # request \u0026 parse rss feed via feedparser  feed=feedparser.parse(feedurl)  for entry in feed[ 'entries' ]:  document_definition = { 'title':entry[ 'title'],  'content':entry['description'],  'permalink':entry[ 'link' ],  'postdate':entry['date'] }   # check if duplicated  documents = list(client.QueryDocuments(  collection['_self'],  {  'query': 'SELECT * FROM root r WHERE r.permalink=@permalink',  'parameters': [  { 'name':'@permalink', 'value':document_definition['permalink'] }  ]  }))  if (len(documents)  1):  # only create if it's fully new document  print \"document is added:title:%s\" % entry['title']  created_document = client.CreateDocument(  collection['_self'], document_definition)   if __name__ == '__main__':  rsscrawling() [実行結果]\ndatabase is created:feeddb collection is created:article_collection document is added:title:業界トップのクラウド アナリストがマイクロソフトに転身した理由 document is added:title:Azure Linux VM のインフラストラクチャの監視と診断 document is added:title:Azure App Service の Web アプリ用 Support Site Extension の追加更新 document is added:title:Azure CDN でカスタムのオリジン サーバーをサポート document is added:title:Tinfoil Security の Azure App Service 向け Web 脆弱性スキャン機能 document is added:title:Azure Site Extensions の自動更新が可能に document is added:title:DocumentDB へのデータのインポートがより高速、簡単に document is added:title:Query Store: データベース版フライト データ レコーダー機能 document is added:title:Azure App Service の Web アプリで使用される TLS の中間証明書 document is added:title:Azure で Cloud Foundry をお試しください document is added:title:Azure Media Indexer の更新版 v1.2.1 をリリース: 前回からの修正点について document is added:title:クラウド環境でエンドツーエンドのビデオ ワークフローを構築する document is added:title:Azure Media Player の更新と UserVoice フォーラムのお知らせ document is added:title:Microsoft Azure 関連ニュース 2015 年 5 月のまとめ document is added:title:Azure Media Services 向け Hyperlapse のパブリック プレビューを発表 document is added:title:6 月 1 日以降の Application Insights の料金設定 document is added:title:StorSimple Update 1 を発表: Azure Government で提供開始、他社クラウドと ZRS をサポート、5000/7000 シリーズからの移行に対応 document is added:title:成功から転落の危機に直面した Sleeve Music。経験豊かな開発者と適切なツールの選択で新たなアイデアを創出 document is added:title:Azure Storage に関する Build 2015 での発表 document is added:title:Visual Studio Code と Azure App Service の連携で実現できること document is added:title:Azure Automation でグラフィック形式とテキスト形式の作成機能を導入 document is added:title:接続性の高い安定したハイブリッド クラウドの実現に向けた新しいネットワーク機能 document is added:title:DocumentDB の新しいインポート オプション document is added:title:Azure Site Recovery が NetApp Private Storage for Microsoft Azure をサポート document is added:title:Application Insights で ASP.NET 5 アプリケーションをサポート ","wordCount":"535","inLanguage":"en","datePublished":"2015-06-21T17:32:40Z","dateModified":"2015-06-21T17:32:40Z","author":{"@type":"Person","name":"Yoichi Kawasaki"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://unofficialism.info/posts/crawler-with-documentdb-python-sdk-and-feedparser/"},"publisher":{"@type":"Organization","name":"Yoichi Kawasaki","logo":{"@type":"ImageObject","url":"https://unofficialism.info/Thinking_Face_Emoji_large.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://unofficialism.info accesskey=h title="unofficialism (Alt + H)">unofficialism</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://unofficialism.info/posts title=posts><span>posts</span></a></li><li><a href=https://unofficialism.info/works title=works><span>works</span></a></li><li><a href=https://unofficialism.info/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://unofficialism.info/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://unofficialism.info>Home</a>&nbsp;»&nbsp;<a href=https://unofficialism.info/posts/>Posts</a></div><h1 class=post-title>DocumentDB Python SDKとfeedparserで作る簡易クローラー</h1><div class=post-meta><span title="2015-06-21 00:00:00 +0000 UTC">June 21, 2015</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Yoichi Kawasaki&nbsp;|&nbsp;<a href=https://github.com/yokawasa/yokawasa.github.io/tree/master/content/posts/2015-06-21-crawler-with-documentdb-python-sdk-and-feedparser.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>DocumentDB Python SDKとfeedparserを使って簡易クローラーを作りましょうというお話。ここではDocumentDBをクローリング結果の格納先データストアとして使用する。クロール対象は<a href=http://blogs.msdn.com/b/windowsazurej/>Azure日本語ブログ</a>の<a href=http://blogs.msdn.com/b/windowsazurej/atom.aspx>RSSフィード</a>、これをfeedparserを使ってドキュメント解析、必要データの抽出、そしてその結果を今回使用するpydocumentdbというDocumentDB Python SDKを使ってDocumentDBに格納するというワークフローになっている。</p><h2 id=documentdb-python-sdk---pydocumentdb>DocumentDB Python SDK - pydocumentdb<a hidden class=anchor aria-hidden=true href=#documentdb-python-sdk---pydocumentdb>#</a></h2><p>Azureで提供されているどのサービスにもあてはまることであるが、DocumentDBを操作するための全てのインターフェースはREST APIとして提供されておりREST APIを内部的に使用してマイクロソフト謹製もしくは個人のコントリビューションによる複数の言語のSDKが用意されている。その中でもpydocumentdbはPython用のDocumentDB SDKであり、オープンソースとしてソースコードは全てGithubで公開されている。</p><ul><li><a href=https://github.com/Azure/azure-documentdb-python>pydocumentdbプロジェクトトップ(Github)</a></li><li><a href=https://github.com/Azure/azure-documentdb-python/blob/master/test/crud_tests.py>pydocumentdbサンプルコード(Github)</a></li><li><a href=https://msdn.microsoft.com/en-us/library/azure/dn781481.aspx>Azure DocumentDB REST API Reference</a></li></ul><h2 id=pre-requirementsその１-python実行環境とライブラリ>Pre-Requirementsその１: Python実行環境とライブラリ<a hidden class=anchor aria-hidden=true href=#pre-requirementsその１-python実行環境とライブラリ>#</a></h2><p>実行環境としてPython2.7系が必要となる。また、今回クローラーが使用しているDocumentDB Python SDKである<a href=https://github.com/Azure/azure-documentdb-python>pydocumentdb</a>とRSSフィード解析ライブラリ<a href=https://github.com/kurtmckee/feedparser>feedparser</a>の２つのライブラリのインストールが必要となる。</p><p>pydocumentdbインストール</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ sudo pip install pydocumentdb
</span></span></code></pre></div><p>feedparserインストール</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ sudo pip install feedparser
</span></span></code></pre></div><p>ちなみにpipがインストールされていない場合は下記の通りマニュアルもしくはインストーラーを使用してpipをインストールが必要となる</p><p>pip マニュアルインストール</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># download get-pip.py</span>
</span></span><span style=display:flex><span>$ wget https://bootstrap.pypa.io/get-pip.py
</span></span><span style=display:flex><span><span style=color:#75715e>#  run the following (which may require administrator access)</span>
</span></span><span style=display:flex><span>$ sudo python get-pip.py
</span></span><span style=display:flex><span><span style=color:#75715e># upgrade pip</span>
</span></span><span style=display:flex><span>$ sudo pip install -U pip
</span></span></code></pre></div><p>インストーラーを使用</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># apt-getの場合</span>
</span></span><span style=display:flex><span>$ sudo apt-get install python-pip
</span></span><span style=display:flex><span><span style=color:#75715e># yumの場合</span>
</span></span><span style=display:flex><span>$ sudo yum install python-pip
</span></span></code></pre></div><h2 id=pre-requirementsその２-documentdbアカウント>Pre-requirementsその２: DocumentDBアカウント<a hidden class=anchor aria-hidden=true href=#pre-requirementsその２-documentdbアカウント>#</a></h2><p>Azure新ポータルより下記イメージのフローでDocumentDBアカウントを作成する。</p><p><img loading=lazy src=https://c1.staticflickr.com/1/345/18984569836_dcbd5e82b7_z.jpg alt=DocumentDB-Account-Create></p><p>DocumentDBアカウントを作成したら今度はデータを格納するためのDocumentDBデータベースとコレクションを作成する必要がある。DocumentDBデータベースとコレクションはAzure新ポータルから作成可能であるが、今回のクローラープログラムではpydocumentdbを使って作成するようにしている。</p><h2 id=rssクローラーとその実行結果>RSSクローラーとその実行結果<a hidden class=anchor aria-hidden=true href=#rssクローラーとその実行結果>#</a></h2><p>RSSクローラーのソースコード（<a href=https://gist.github.com/yokawasa/e41c1517700ebc6f67df>rssCrawler4Docdb.py</a>）とその実行結果は下記の通り。もしクローリングを定期的に実行する場合はこのスクリプトをジョブスケジューラー（cronなど）に登録ください。指定のRSSフィードに新しく追加された記事のみがDocumentDBに反映されるようスクリプト上は重複チェックを入れている。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- coding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> feedparser
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pydocumentdb.documents <span style=color:#66d9ef>as</span> documents
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pydocumentdb.document_client <span style=color:#66d9ef>as</span> document_client
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pydocumentdb.errors <span style=color:#66d9ef>as</span> errors
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pydocumentdb.http_constants <span style=color:#66d9ef>as</span> http_constants
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Docdb_masterKey <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&lt;Your Documentdb master key string&gt;&#39;</span>
</span></span><span style=display:flex><span>Docdb_host <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://&lt;documentdb account&gt;.documents.azure.com:443/&#39;</span>
</span></span><span style=display:flex><span>Docdb_dbname <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&lt;documentdb database name&gt;&#39;</span>
</span></span><span style=display:flex><span>Docdb_colname <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&lt;documentdb collection name&gt;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>feedurl<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;http://blogs.msdn.com/b/windowsazurej/atom.aspx&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>rsscrawling</span>():
</span></span><span style=display:flex><span>    <span style=color:#75715e># create documentDb client instance</span>
</span></span><span style=display:flex><span>    client <span style=color:#f92672>=</span> document_client<span style=color:#f92672>.</span>DocumentClient(Docdb_host,
</span></span><span style=display:flex><span>                                 {<span style=color:#e6db74>&#39;masterKey&#39;</span>: Docdb_masterKey})
</span></span><span style=display:flex><span>    <span style=color:#75715e># create a database if not yet created</span>
</span></span><span style=display:flex><span>    database_definition <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;id&#39;</span>: Docdb_dbname }
</span></span><span style=display:flex><span>    databases <span style=color:#f92672>=</span> list(client<span style=color:#f92672>.</span>QueryDatabases({
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;query&#39;</span>: <span style=color:#e6db74>&#39;SELECT * FROM root r WHERE r.id=@id&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;parameters&#39;</span>: [
</span></span><span style=display:flex><span>                { <span style=color:#e6db74>&#39;name&#39;</span>:<span style=color:#e6db74>&#39;@id&#39;</span>, <span style=color:#e6db74>&#39;value&#39;</span>: database_definition[<span style=color:#e6db74>&#39;id&#39;</span>] }
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        }))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> ( len(databases) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> ):
</span></span><span style=display:flex><span>        feeddb <span style=color:#f92672>=</span> databases[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        print <span style=color:#e6db74>&#34;database is created:</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> Docdb_dbname
</span></span><span style=display:flex><span>        feeddb <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>CreateDatabase(database_definition)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># create a collection if not yet created</span>
</span></span><span style=display:flex><span>    collection_definition <span style=color:#f92672>=</span> { <span style=color:#e6db74>&#39;id&#39;</span>: Docdb_colname }
</span></span><span style=display:flex><span>    collections <span style=color:#f92672>=</span> list(client<span style=color:#f92672>.</span>QueryCollections(
</span></span><span style=display:flex><span>        feeddb[<span style=color:#e6db74>&#39;_self&#39;</span>],
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;query&#39;</span>: <span style=color:#e6db74>&#39;SELECT * FROM root r WHERE r.id=@id&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;parameters&#39;</span>: [
</span></span><span style=display:flex><span>                { <span style=color:#e6db74>&#39;name&#39;</span>:<span style=color:#e6db74>&#39;@id&#39;</span>, <span style=color:#e6db74>&#39;value&#39;</span>: collection_definition[<span style=color:#e6db74>&#39;id&#39;</span>] }
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        }))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> ( len(collections) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> ):
</span></span><span style=display:flex><span>        collection <span style=color:#f92672>=</span> collections[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        print <span style=color:#e6db74>&#34;collection is created:</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> Docdb_colname
</span></span><span style=display:flex><span>        collection <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>CreateCollection(
</span></span><span style=display:flex><span>                    feeddb[<span style=color:#e6db74>&#39;_self&#39;</span>], collection_definition)
</span></span><span style=display:flex><span>    <span style=color:#75715e># request &amp; parse rss feed via feedparser</span>
</span></span><span style=display:flex><span>    feed<span style=color:#f92672>=</span>feedparser<span style=color:#f92672>.</span>parse(feedurl)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> entry <span style=color:#f92672>in</span> feed[ <span style=color:#e6db74>&#39;entries&#39;</span> ]:
</span></span><span style=display:flex><span>        document_definition <span style=color:#f92672>=</span> { <span style=color:#e6db74>&#39;title&#39;</span>:entry[ <span style=color:#e6db74>&#39;title&#39;</span>],
</span></span><span style=display:flex><span>                                <span style=color:#e6db74>&#39;content&#39;</span>:entry[<span style=color:#e6db74>&#39;description&#39;</span>],
</span></span><span style=display:flex><span>                                <span style=color:#e6db74>&#39;permalink&#39;</span>:entry[ <span style=color:#e6db74>&#39;link&#39;</span> ],
</span></span><span style=display:flex><span>                                <span style=color:#e6db74>&#39;postdate&#39;</span>:entry[<span style=color:#e6db74>&#39;date&#39;</span>] }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># check if duplicated</span>
</span></span><span style=display:flex><span>        documents <span style=color:#f92672>=</span> list(client<span style=color:#f92672>.</span>QueryDocuments(
</span></span><span style=display:flex><span>            collection[<span style=color:#e6db74>&#39;_self&#39;</span>],
</span></span><span style=display:flex><span>            {
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#39;query&#39;</span>: <span style=color:#e6db74>&#39;SELECT * FROM root r WHERE r.permalink=@permalink&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#39;parameters&#39;</span>: [
</span></span><span style=display:flex><span>                    { <span style=color:#e6db74>&#39;name&#39;</span>:<span style=color:#e6db74>&#39;@permalink&#39;</span>, <span style=color:#e6db74>&#39;value&#39;</span>:document_definition[<span style=color:#e6db74>&#39;permalink&#39;</span>] }
</span></span><span style=display:flex><span>                ]
</span></span><span style=display:flex><span>            }))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (len(documents) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>            <span style=color:#75715e># only create if it&#39;s fully new document</span>
</span></span><span style=display:flex><span>            print <span style=color:#e6db74>&#34;document is added:title:</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> entry[<span style=color:#e6db74>&#39;title&#39;</span>]
</span></span><span style=display:flex><span>            created_document <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>CreateDocument(
</span></span><span style=display:flex><span>                    collection[<span style=color:#e6db74>&#39;_self&#39;</span>], document_definition)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    rsscrawling()
</span></span></code></pre></div><p>[実行結果]</p><pre tabindex=0><code>database is created:feeddb
collection is created:article_collection
document is added:title:業界トップのクラウド アナリストがマイクロソフトに転身した理由
document is added:title:Azure Linux VM のインフラストラクチャの監視と診断
document is added:title:Azure App Service の Web アプリ用 Support Site Extension の追加更新
document is added:title:Azure CDN でカスタムのオリジン サーバーをサポート
document is added:title:Tinfoil Security の Azure App Service 向け Web 脆弱性スキャン機能
document is added:title:Azure Site Extensions の自動更新が可能に
document is added:title:DocumentDB へのデータのインポートがより高速、簡単に
document is added:title:Query Store: データベース版フライト データ レコーダー機能
document is added:title:Azure App Service の Web アプリで使用される TLS の中間証明書
document is added:title:Azure で Cloud Foundry をお試しください
document is added:title:Azure Media Indexer の更新版 v1.2.1 をリリース: 前回からの修正点について
document is added:title:クラウド環境でエンドツーエンドのビデオ ワークフローを構築する
document is added:title:Azure Media Player の更新と UserVoice フォーラムのお知らせ
document is added:title:Microsoft Azure 関連ニュース 2015 年 5 月のまとめ
document is added:title:Azure Media Services 向け Hyperlapse のパブリック プレビューを発表
document is added:title:6 月 1 日以降の Application Insights の料金設定
document is added:title:StorSimple Update 1 を発表: Azure Government で提供開始、他社クラウドと ZRS をサポート、5000/7000 シリーズからの移行に対応
document is added:title:成功から転落の危機に直面した Sleeve Music。経験豊かな開発者と適切なツールの選択で新たなアイデアを創出
document is added:title:Azure Storage に関する Build 2015 での発表
document is added:title:Visual Studio Code と Azure App Service の連携で実現できること
document is added:title:Azure Automation でグラフィック形式とテキスト形式の作成機能を導入
document is added:title:接続性の高い安定したハイブリッド クラウドの実現に向けた新しいネットワーク機能
document is added:title:DocumentDB の新しいインポート オプション
document is added:title:Azure Site Recovery が NetApp Private Storage for Microsoft Azure をサポート
document is added:title:Application Insights で ASP.NET 5 アプリケーションをサポート
</code></pre></div><footer class=post-footer><ul class=post-tags><li><a href=https://unofficialism.info/tags/documentdb/>DocumentDB</a></li><li><a href=https://unofficialism.info/tags/nosql/>NoSQL</a></li><li><a href=https://unofficialism.info/tags/python/>Python</a></li><li><a href=https://unofficialism.info/tags/crawler/>Crawler</a></li><li><a href=https://unofficialism.info/tags/pydocumentdb/>pydocumentdb</a></li><li><a href=https://unofficialism.info/tags/feedparser/>feedparser</a></li></ul><nav class=paginav><a class=prev href=https://unofficialism.info/posts/levelage-documentdb-as-azuresearch-datasource/><span class=title>« Prev Page</span><br><span>DocumentDBをAzure Searchのデータソースとして利用する</span></a>
<a class=next href=https://unofficialism.info/posts/putting-wikipedia-data-into-azure-search/><span class=title>Next Page »</span><br><span>Wikipediaデータベースを元にAzure Searchインデックスを生成する</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share DocumentDB Python SDKとfeedparserで作る簡易クローラー on twitter" href="https://twitter.com/intent/tweet/?text=DocumentDB%20Python%20SDK%e3%81%a8feedparser%e3%81%a7%e4%bd%9c%e3%82%8b%e7%b0%a1%e6%98%93%e3%82%af%e3%83%ad%e3%83%bc%e3%83%a9%e3%83%bc&url=https%3a%2f%2funofficialism.info%2fposts%2fcrawler-with-documentdb-python-sdk-and-feedparser%2f&hashtags=DocumentDB%2cNoSQL%2cPython%2cCrawler%2cpydocumentdb%2cfeedparser"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share DocumentDB Python SDKとfeedparserで作る簡易クローラー on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2funofficialism.info%2fposts%2fcrawler-with-documentdb-python-sdk-and-feedparser%2f&title=DocumentDB%20Python%20SDK%e3%81%a8feedparser%e3%81%a7%e4%bd%9c%e3%82%8b%e7%b0%a1%e6%98%93%e3%82%af%e3%83%ad%e3%83%bc%e3%83%a9%e3%83%bc&summary=DocumentDB%20Python%20SDK%e3%81%a8feedparser%e3%81%a7%e4%bd%9c%e3%82%8b%e7%b0%a1%e6%98%93%e3%82%af%e3%83%ad%e3%83%bc%e3%83%a9%e3%83%bc&source=https%3a%2f%2funofficialism.info%2fposts%2fcrawler-with-documentdb-python-sdk-and-feedparser%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share DocumentDB Python SDKとfeedparserで作る簡易クローラー on reddit" href="https://reddit.com/submit?url=https%3a%2f%2funofficialism.info%2fposts%2fcrawler-with-documentdb-python-sdk-and-feedparser%2f&title=DocumentDB%20Python%20SDK%e3%81%a8feedparser%e3%81%a7%e4%bd%9c%e3%82%8b%e7%b0%a1%e6%98%93%e3%82%af%e3%83%ad%e3%83%bc%e3%83%a9%e3%83%bc"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share DocumentDB Python SDKとfeedparserで作る簡易クローラー on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2funofficialism.info%2fposts%2fcrawler-with-documentdb-python-sdk-and-feedparser%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://unofficialism.info>Yoichi Kawasaki</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>